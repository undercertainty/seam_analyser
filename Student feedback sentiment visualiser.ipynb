{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEaM free text sentiment visualiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by selecting a file. (The widget seems to need you to click on `Select` before it actually starts properly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "fc=FileChooser()\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull in the data and import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the data doesn't appear to be in a standard format. I'll continue to assume that the student responses are all in sheet 2, but I'll try to suck out the columns containing free text. Again, `NLTK` is probably the best approach for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And stop pandas from curtailing the outputs so we can see the whole text cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get a SEaM data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this line to read the seam file\n",
    "\n",
    "seam_df=pd.read_excel(fc.selected, sheet_name=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the contents of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seam_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter the columns to the text columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There probably isn't an especially principled way of distinguishing the text columns from the non-text columns... Let's create a set of the terms which appear in the question columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_terms={'1.',\n",
    "                '2.',\n",
    "                '3.',\n",
    "                '4.',\n",
    "                '5.',\n",
    "                'definitely',\n",
    "                'mostly',\n",
    "                'neither',\n",
    "                'student',\n",
    "                'agree',\n",
    "                'answer',\n",
    "                'did',\n",
    "                'disagree',\n",
    "                'nor',\n",
    "                'not',\n",
    "                'question',\n",
    "                'this'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and assume that a column represents text if it contains several terms that aren't in this list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll create a function `text_series` which returns `True` if the values contain several terms which aren't in the list of question terms.\n",
    "\n",
    "Let's put a threshold of 10. So it's true if there are more than 10 different non-question terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_series(s_in):\n",
    "    '''return True if the series' values contain, say, 5 distinct terms which\n",
    "       aren't in question_terms'''\n",
    "    \n",
    "    terms=(s_in\n",
    "            \n",
    "           .dropna()\n",
    "           \n",
    "           .str.lower()\n",
    "           \n",
    "           .str.split()\n",
    "          \n",
    "           .values)\n",
    "    \n",
    "    terms=[x.strip(string.punctuation+'0123456789') for y in terms\n",
    "              for x in y]\n",
    "    \n",
    "    terms={term for term in terms\n",
    "           if term not in question_terms\n",
    "           and term}   # remove ''\n",
    "    \n",
    "    return len(terms)>=10\n",
    "\n",
    "text_series(seam_df['If you answered Disagree to any of the statements above, we would like to understand why so we can make improvements in the future'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback_df=seam_df.filter([c for c in seam_df.columns if text_series(seam_df[c])])\n",
    "\n",
    "feedback_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks about right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we should be able to do the splitting and merging thing on these columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the sentences in the free text cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the input into separate sentences, use the NLTK library function `sent_tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the language model for sentence splitting\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can put all the sentences into a single DataFrame. Reasonably tidily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df=pd.DataFrame(columns=['response', 'sentence_num'])\n",
    "\n",
    "for text_column in feedback_df.columns:\n",
    "\n",
    "    ss=(feedback_df[text_column]\n",
    "        .dropna())\n",
    "\n",
    "    l=[]\n",
    "\n",
    "    for (idx, text) in ss.items():\n",
    "        l.extend([{'response':idx, 'sentence_num':i, text_column:s}\n",
    "                  for (i, s) in enumerate(sent_tokenize(ss[idx]))])\n",
    "        \n",
    "    all_comments_df=all_comments_df.merge(pd.DataFrame(l), how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small quirk: the ordering seems to have gone awry in places, so let's just make sure it's sorted properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_df.sort_values(by=['response', 'sentence_num'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the sentiment analyser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Vader sentiment analyser from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the language model for sentiment analysis\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"TM351 was the best module I have ever imagined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\"TM351 is the worst course I have studied in decades at the OU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'compound'` key in the dictionary is the one we want: range from -1 to +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the power of *seaborn*, which generates nice graded palettes, with *pandas*'  styling methods for DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use the palette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_colour_map=sns.diverging_palette(10, 125, s=75, l=50,\n",
    "                                           n=12, center=\"light\", as_cmap=True)\n",
    "sentiment_colour_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure why they've got the central values there as \"bad\". Still..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then map the sentences in the DataFrame onto the `compound` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def polarity_scores_check(txt):\n",
    "    '''Returns the result of polarity_scores, but with 0 for cases\n",
    "       raising an error (avoids throwing errors for NaNs and the\n",
    "       like).\n",
    "    '''\n",
    "    try:\n",
    "        return sia.polarity_scores(txt)['compound']\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "all_comments_df.applymap(polarity_scores_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can use the polarity scores DataFrame to colour the cells in the text DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_comments_df.style.background_gradient(cmap=sentiment_colour_map,\n",
    "                                         axis=None, vmin=-1, vmax=1,\n",
    "                                          gmap=all_comments_df.applymap(polarity_scores_check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
